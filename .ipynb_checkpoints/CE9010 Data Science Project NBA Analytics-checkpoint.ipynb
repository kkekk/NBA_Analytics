{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CE9010 Introduction to Data Science Project\n",
    "\n",
    "# Data Scraper\n",
    "\n",
    "\n",
    "\n",
    "First, we import the relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import Comment, BeautifulSoup as bs\n",
    "import urllib.request\n",
    "import csv\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO BE FORMATTED \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\n",
    "\n",
    "Data Acquisition\n",
    " ------\n",
    "\n",
    "#### Warning, long runtime of up to 40 minutes. Program will finish scraping when END OF YEAR: 2016 is printed.\n",
    "\n",
    "Scraper written in beautifulsoup.\n",
    "\n",
    "#### Types of data extracted\n",
    "---\n",
    "\n",
    "Our main goal of our is to predict the impact of college prospects in the first and second year of NBA based on their college stats. Hence we choose to use their advanced stats available in college (ws, ws/40min) to generalize the impact on NBA measured in their advanced stats available in the NBA (per, ws, ws/48, bpm, vorp) for their first two years in the league. We chose to extract data from 1996 as college advanced statistics were only available from the 1995 season onwards. We also chose to only include the first 30 draft picks of each draft as players beyond the 30th picks usually do not play significant minutes in their teams.\n",
    "\n",
    "Player's NBA stats extracted from [Basketball Reference](https://www.basketball-reference.com/).\n",
    "\n",
    "Player's College stats extracted from [Sports Reference](https://www.sports-reference.com/)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "#### Problems encountered\n",
    "\n",
    "---\n",
    "\n",
    "The main problem encountered when running the scraper is the long runtime for a relatively small dataset est <200? confirm again>. The bottleneck occurs when parsing the whole page to look for the link to the sports reference website for each player in order to acquire their respective college stats. Having to parse the whole page for every player leads to a long runtime; inefficient code. A better solution would be to use another data scraping framework such as Scrapy which allows data extraction using [selectors](https://doc.scrapy.org/en/latest/topics/selectors.html). Extracting the link to sports-reference website of the page using the XPath or CSS selector would be much quicker than parsing the entire page. However as this scraper would only need to be run once it is not a major concern to optimize and rewrite another scraper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_comments(inside_html, html_id):\n",
    "    comments = inside_html.findAll(text=lambda text:isinstance(text, Comment)) #data we want is commented, hence the need \n",
    "    comments = [comment.extract() for comment in comments if 'id=\\\"' + html_id + '\\\"' in comment.extract()] #get advanced stats table\n",
    "    return bs(comments[0], 'html.parser')\n",
    "\n",
    "def read_url_into_soup(url):\n",
    "    try:\n",
    "        next_page = urllib.request.urlopen(url).read() # goes to player page\n",
    "        return bs(next_page, 'html.parser')\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "        return read_url_into_soup(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allen Iverson\n",
      "['Allen Iverson', '9.83', 1996, '1213', '.578', '.547', '.366', '.488', '.283', '37', '37', '10.3', '21.4', '.480', '7.4', '13.6', '.546', '2.9', '7.8', '.366', '7.1', '10.5', '.678', '4.6', '5.7', '4.1', '0.5', '4.6', '2.9', '30.5']\n"
     ]
    }
   ],
   "source": [
    "f = open(\"data/nba_all.csv\", \"w\")\n",
    "f2 = open(\"data/college_all.csv\", \"w\")\n",
    "writer = csv.writer(f)\n",
    "writerf = csv.writer(f2)    \n",
    "running = False\n",
    "# year_dict = {}\n",
    "\n",
    "for year in range(1996, 2017):\n",
    "    url = \"https://www.basketball-reference.com/draft/NBA_\"+ str(year) + \".html\"\n",
    "#     req = urllib.request.Request(url)\n",
    "#     html = read_url_into_soup(req)\n",
    "    html = read_url_into_soup(url)\n",
    "#     response = urllib.request.urlopen(req)\n",
    "#     html = response.read()\n",
    "#     html = bs(html, 'html.parser')\n",
    "    if running:\n",
    "        break\n",
    "    \n",
    "    for row in html.table.tbody.findAll(\"tr\"):\n",
    "\n",
    "        if (row.find(\"td\")) is None:\n",
    "            print(\"END OF YEAR: \"+ str(year) +\"\\n\")  # reach end of file\n",
    "            break\n",
    "            \n",
    "        seasons = row.find(\"td\", {\"data-stat\" : \"seasons\"} )\n",
    "        \n",
    "        if (not seasons.get_text() or  #if player did not play in NBA after being drafted\n",
    "        int(seasons.get_text()) < 2 or  # if less than 2 seasons played, skip\n",
    "        len(row.find(\"td\", {\"data-stat\" : \"college_name\"}) ) < 1): #if player played in euroleague\n",
    "            continue\n",
    "            \n",
    "        player = row.find(\"td\",{\"data-stat\" : \"player\"})\n",
    "        print(player.get_text())\n",
    "        inside_html = read_url_into_soup(\"https://www.basketball-reference.com/\" + player.a['href'])\n",
    "#         next_page = urllib.request.urlopen(\"https://www.basketball-reference.com/\" + player.a['href']).read() # goes to player page\n",
    "#         inside_html = bs(next_page, 'html.parser')\n",
    "        \n",
    "        advanced = extract_comments(inside_html, 'advanced')\n",
    "        \n",
    "        # STATS\n",
    "          \n",
    "        out = [player.string]\n",
    "        cols = ['per', 'ws_per_48', 'bpm', 'vorp'] # stats to include\n",
    "        for col in cols:\n",
    "            out.append(advanced.findAll('td', {'data-stat' : col})[0].string) #include both 1st and 2nd year\n",
    "            out.append(advanced.findAll('td', {'data-stat' : col})[1].string) \n",
    "        out.append(year)\n",
    "        writer.writerow(out)\n",
    "        \n",
    "        for i in inside_html.findAll(\"a\"): #inefficient way of finding the url for college stats\n",
    "            if \"College Basketball\" in str(i):\n",
    "                coll_url = i['href']\n",
    "                break\n",
    "              \n",
    "        coll_html = read_url_into_soup(coll_url)\n",
    "#         coll_page = urllib.request.urlopen(coll_url).read()\n",
    "#         coll_html = bs(coll_page, 'html.parser')\n",
    "\n",
    "        \n",
    "        coll_advanced = extract_comments(coll_html, 'players_advanced')\n",
    "        coll_html = read_url_into_soup(coll_url)\n",
    "        coll_players_pm = extract_comments(coll_html, 'players_per_min')\n",
    "        \n",
    "        \n",
    "        cols = ['g','gs','fg_per_min','fga_per_min','fg_pct','fg2_per_min','fg2a_per_min','fg2_pct','fg3_per_min',\n",
    "                'fg3a_per_min','fg3_pct','ft_per_min','fta_per_min','ft_pct','trb_per_min','ast_per_min', 'stl_per_min',\n",
    "                'blk_per_min', 'tov_per_min','pf_per_min','pts_per_min']\n",
    "\n",
    "        pm = [coll_players_pm.tbody.findAll('td', {'data-stat' : c})[-1].string for c in cols]\n",
    "        \n",
    "        #input stats\n",
    "        cols = [\"mp\", \"ts_pct\",\"efg_pct\",\"fg3a_per_fga_pct\", \"fta_per_fga_pct\",\"ws_per_40\"]\n",
    "#         for col in cols:\n",
    "#         ws40 = coll_advanced.tbody.findAll('td', {'data-stat' : 'ws_per_40'})[-1].string\n",
    "        adv = [coll_advanced.tbody.findAll('td', {'data-stat' : c})[-1].string for c in cols]\n",
    "        \n",
    "        sos = coll_html.findAll('td', {'data-stat' : 'sos'})[-2].string\n",
    "        result = [player.string, sos, year]\n",
    "        result.extend(adv)\n",
    "        result.extend(pm)\n",
    "#         print(result)\n",
    "        writerf.writerow(result)\n",
    "#         running = True\n",
    "#         break\n",
    "            \n",
    "f.close()\n",
    "f2.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML SHIT HERE\n",
    "#### We used college_all and nba_all outside the data/ directory in case we accidentally ran the above function and everything is overwritten\n",
    "\n",
    "- remove rows with NaN (somehow in 1997-1998 some college statistics were not recorded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 5)\n",
      "(486, 15)\n",
      "(440, 15)\n",
      "Are there any rows with empty cells? False\n"
     ]
    }
   ],
   "source": [
    "college_df = pd.read_csv('college_all.csv', header=None)\n",
    "college_df = college_df.rename(index=str, columns={0: \"Player\"})\n",
    "# college_df.columns = [\"Player\",\"sos\", \"year\",\"mp\", \"ts_pct\",\"efg_pct\",\"fg3a_per_fga_pct\", \"fta_per_fga_pct\",\"ws_per_40\",\n",
    "#                       'g','gs','fg_per_min','fga_per_min','fg_pct','fg2_per_min','fg2a_per_min','fg2_pct','fg3_per_min',\n",
    "#                 'fg3a_per_min','fg3_pct','ft_per_min','fta_per_min','ft_pct','trb_per_min','ast_per_min', 'stl_per_min',\n",
    "#                 'blk_per_min', 'tov_per_min','pf_per_min','pts_per_min']\n",
    "nba_df = pd.read_csv('nba_all.csv', header=None)\n",
    "nba_df.columns = ['Player','PER 1st', 'PER 2nd', 'WS 1st','WS 2nd', 'WS48 1st','WS48 2nd', 'BPM 1st', 'BPM 2nd','VORP 1st', 'VORP 2nd', 'year']\n",
    "\n",
    "nba_df = nba_df.drop(['year'], axis=1) #remove duplicate column\n",
    "\n",
    "comb = pd.merge(nba_df, college_df, on=['Player', 'Player'])\n",
    "\n",
    "print(college_df[college_df.isnull().any(axis=1)].shape) #amount of data to remove\n",
    "print(comb.shape)\n",
    "comb = comb.dropna(how = 'any') #remove rows with NaN\n",
    "print(comb.shape)\n",
    "print(\"Are there any rows with empty cells?\", comb.isnull().values.any() )\n",
    "\n",
    "# comb = comb.sort_values(by = [\"College Win Shares\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    " #enter favourite number\n",
    "seed = 100\n",
    "  \n",
    "def split_val_set(y_var, no_of_set):\n",
    "    '''\n",
    "    SPLITS TRAIN SET INTO VAL AND TRAINING SETS\n",
    "    '''\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(comb[[\"College Win Shares\", \"College Win Shares per 40 min\", \"College Strength of Schedule\", \"Year Drafted\"]], comb[y_var], test_size=0.20, random_state=seed)\n",
    "\n",
    "    splits_x = []\n",
    "    splits_y = []\n",
    "    no_of_set = 4\n",
    "    size_set = int(len(X_train) / no_of_set)\n",
    "    for i in range(0, no_of_set - 1):\n",
    "        split_set_x = X_train[size_set * i : size_set * (i+1)] #split first 3 evenly\n",
    "        split_set_y = y_train[size_set * i : size_set * (i+1)]\n",
    "        splits_x.append(split_set_x)\n",
    "        splits_y.append(split_set_y)\n",
    "    splits_x.append(X_train[size_set*3:]) \n",
    "    splits_y.append(y_train[size_set*3:])\n",
    "    \n",
    "    return splits_x, splits_y, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for sklearn\n",
    "normalize : boolean, optional, default False\n",
    "\n",
    "This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit on an estimator with normalize=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nba_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-1c661c55627f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mno_of_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0my_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnba_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mxset\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0myset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_val_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_of_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nba_df' is not defined"
     ]
    }
   ],
   "source": [
    "lin_reg_sklearn = LinearRegression(normalize = True) #normalize according to z\n",
    "\n",
    "no_of_set = 4\n",
    "for y_name in list(nba_df):\n",
    "    print(y_name)\n",
    "    xset , yset, x_test, y_test = split_val_set(y_name, no_of_set)\n",
    "    # comb['College Win Shares per 40 min'] = preprocessing.scale(comb['College Win Shares per 40 min']\n",
    "    \n",
    "    \n",
    "    print(len(xset),len(yset))\n",
    "\n",
    "    for i in range(no_of_set):\n",
    "        xset[i]\n",
    "        lin_reg_sklearn.fit(comb['College Win Shares'].reshape([-1,1]), comb['Win Shares 2nd'].reshape([-1,1]))\n",
    "        w_sklearn = np.zeros([2,1])\n",
    "        w_sklearn[0,0] = lin_reg_sklearn.intercept_\n",
    "        w_sklearn[1,0] = lin_reg_sklearn.coef_\n",
    "        print(w_sklearn)\n",
    "        plt.scatter(comb['College Win Shares per 40 min'], comb['Win Shares 2nd'],s=20, c='r', marker='o', linewidths=1)\n",
    "        # plt.plot(range(-5,6), np.asarray([lin_reg_sklearn.predict(x) for x in range(-5,6)]).reshape(-1 ,1))\n",
    "        # y_pred_sklearn = w_sklearn[0] + w_sklearn[1]* \n",
    "        plt.plot()\n",
    "        # plt.plot(comb['College Win Shares per 40 min'], np.asarray([lin_reg_sklearn.predict(x) for x in comb['College Win Shares per 40 min']]).reshape(-1 ,1))\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>PER 1st</th>\n",
       "      <th>PER 2nd</th>\n",
       "      <th>Win Shares 1st</th>\n",
       "      <th>Win Shares 2nd</th>\n",
       "      <th>Win Shares per 48 min 1st</th>\n",
       "      <th>Win Shares per 48 min 2nd</th>\n",
       "      <th>BPM 1st</th>\n",
       "      <th>BPM 2nd</th>\n",
       "      <th>VORP 1st</th>\n",
       "      <th>VORP 2nd</th>\n",
       "      <th>College Win Shares</th>\n",
       "      <th>College Win Shares per 40 min</th>\n",
       "      <th>College Strength of Schedule</th>\n",
       "      <th>Year Drafted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Allen Iverson</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.4</td>\n",
       "      <td>4.1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.138</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.6</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.283</td>\n",
       "      <td>9.83</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marcus Camby</td>\n",
       "      <td>17.8</td>\n",
       "      <td>15.9</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.320</td>\n",
       "      <td>8.92</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shareef Abdur-Rahim</td>\n",
       "      <td>17.4</td>\n",
       "      <td>21.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.221</td>\n",
       "      <td>6.44</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stephon Marbury</td>\n",
       "      <td>16.1</td>\n",
       "      <td>16.3</td>\n",
       "      <td>3.7</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.1</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.127</td>\n",
       "      <td>12.71</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ray Allen</td>\n",
       "      <td>14.6</td>\n",
       "      <td>16.2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.303</td>\n",
       "      <td>8.19</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Player  PER 1st  PER 2nd  Win Shares 1st  Win Shares 2nd  \\\n",
       "0        Allen Iverson     18.0     20.4             4.1             9.0   \n",
       "1         Marcus Camby     17.8     15.9             3.7             0.9   \n",
       "2  Shareef Abdur-Rahim     17.4     21.1             2.9             6.9   \n",
       "3      Stephon Marbury     16.1     16.3             3.7             5.3   \n",
       "4            Ray Allen     14.6     16.2             4.9             7.0   \n",
       "\n",
       "   Win Shares per 48 min 1st  Win Shares per 48 min 2nd  BPM 1st  BPM 2nd  \\\n",
       "0                      0.065                      0.138      1.5      3.8   \n",
       "1                      0.095                      0.022     -0.3     -0.7   \n",
       "2                      0.049                      0.113     -2.0      1.2   \n",
       "3                      0.077                      0.082     -1.0     -0.6   \n",
       "4                      0.092                      0.102      0.3      1.8   \n",
       "\n",
       "   VORP 1st  VORP 2nd  College Win Shares  College Win Shares per 40 min  \\\n",
       "0       2.7       4.6                 8.6                          0.283   \n",
       "1       0.8       0.7                 8.1                          0.320   \n",
       "2       0.0       2.3                 5.4                          0.221   \n",
       "3       0.6       1.1                 4.3                          0.127   \n",
       "4       1.5       3.2                 8.3                          0.303   \n",
       "\n",
       "   College Strength of Schedule  Year Drafted  \n",
       "0                          9.83          1996  \n",
       "1                          8.92          1996  \n",
       "2                          6.44          1996  \n",
       "3                         12.71          1996  \n",
       "4                          8.19          1996  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RECIPE TO FOLLOW\n",
    "\n",
    "## Pre process (zero mean? unit variance?)\n",
    "## Extract a subset of training data and over fit them? L train close to zero and Lval high by manually selecting hyper par\n",
    "## Add regularization and evaluate the generalization performance on the validation set\n",
    "## Lval and L train gap should be minimized and both be ideally small\n",
    "## USe all training data and cross validation to estimate the parameters and hyper parameters \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
